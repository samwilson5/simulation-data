{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load in the site coordinates, labels and states"
      ],
      "metadata": {
        "id": "8OogTfT1yA5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqPDFdncUInH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Thesis/Chapter2_simulations/locations/points_States.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX-MD-YNfvwc"
      },
      "outputs": [],
      "source": [
        "cal_df = df.loc[df['STUSPS']=='CA']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz4QPthcUdHn"
      },
      "outputs": [],
      "source": [
        "site = cal_df['Label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBfvwEUP1UIy"
      },
      "source": [
        "need to swith from negative to positive coordinates for Livneh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU9Dk7Y3UiCc"
      },
      "outputs": [],
      "source": [
        "lon = 360+cal_df['X_WGS84']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRywnoSWUoXG"
      },
      "outputs": [],
      "source": [
        "lat = cal_df['Y_WGS84']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of all the GCM's used for this simulation"
      ],
      "metadata": {
        "id": "xgSqOEsgyGh4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcgfprepLZbu"
      },
      "outputs": [],
      "source": [
        "gcms=['bcc-csm1-1','bcc-csm1-1-m','BNU-ESM','CanESM2','CCSM4','CNRM-CM5',\n",
        "      'CSIRO-Mk3-6-0','GFDL-ESM2G','GFDL-ESM2M','HadGEM2-CC365','HadGEM2-ES365',\n",
        "      'inmcm4','IPSL-CM5A-LR','IPSL-CM5A-MR','IPSL-CM5B-LR','MIROC5','MIROC-ESM',\n",
        "      'MIROC-ESM-CHEM','MRI-CGCM3','NorESM1-M']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIMrA6CuMs4K"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "#ppt_files = glob.glob('/content/drive/MyDrive/Thesis/Chapter2_simulations/netcdfs/CA/agg_macav2metdata_pr_'+gcms[1]+'*rcp45_*.nc')\n",
        "#tmax_files = glob.glob('/content/drive/MyDrive/Thesis/Chapter2_simulations/netcdfs/CA/agg_macav2metdata_tasmax_'+gcms[1]+'*rcp45_*.nc')\n",
        "#tmin_files = glob.glob('/content/drive/MyDrive/Thesis/Chapter2_simulations/netcdfs/CA/agg_macav2metdata_tasmin_'+gcms[1]+'*rcp45_*.nc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz4-8RWfN5SY"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "#exam = xr.open_dataset(ppt_files[0])\n",
        "# Need to get rid of leap years\n",
        "#exam = exam.sel(time=~((exam.time.dt.month == 2) & (exam.time.dt.day == 29)))\n",
        "#big_exam = exam.sel(lon=xr.DataArray(lon), lat=xr.DataArray(lat), method='nearest')\n",
        "#big_exam = big_exam.to_array()\n",
        "#big_exam = big_exam.reindex(space=['dim_0'])\n",
        "#exam_df = pd.DataFrame(big_exam[0,:,:])\n",
        "#t_exam_df = exam_df.transpose()\n",
        "#t_exam_df['site'] = site"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gU2Eub4pQjV7"
      },
      "outputs": [],
      "source": [
        "#x = pd.DataFrame(t_exam_df.iloc[:,:-1].values.ravel(), columns = ['PPT'])\n",
        "#years = list(range(2006,2100))\n",
        "#days = list(range(1,366))\n",
        "#site = cal_df['Label']\n",
        "#years = [ele for ele in years for j in range(365)]\n",
        "#site =  [ele for ele in site for j in range(365*94)]\n",
        "#days = [ele for ele in days for j in range(94)]\n",
        "#x['site'] = [ele for ele in site for j in range(len(exam_df.axes[1]))]\n",
        "#x['site'] = site\n",
        "#x['year'] = years*len(exam_df.axes[1])\n",
        "#x['doy'] = days*(94*len(exam_df.axes[1]))\n",
        "#x['doy'] = [ele for ele in days for j in range(898*94)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The files had to be seperated into state in order for them to be small enough to work with. Therefore, the list of states below will be looped through when globbing the nc files"
      ],
      "metadata": {
        "id": "Bonr9-5PyPXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqGXvum7mql1"
      },
      "outputs": [],
      "source": [
        "states = ['WA','OR','CA','NV','NM','AZ','UT','ID','MT','WY','CO']\n",
        "#states = ['NV','NM','AZ','UT','ID','MT','WY','CO']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop below takes in a certain RCP (4.5 and 8.5 for this study) and loops through all western files. It then creates a csv for each state and each GCM with rows (days) having tmin, tmax, and ppt for each site for all years."
      ],
      "metadata": {
        "id": "L3xhKKB9ybhk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPATWytdjtZq",
        "outputId": "46c5f47b-47a7-44d7-a1c2-174b44d775ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/io/_netcdf.py:305: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\n",
            "  warnings.warn((\n"
          ]
        }
      ],
      "source": [
        "import xarray as xr\n",
        "import glob\n",
        "for j in states:\n",
        "  t_df = df.loc[df['STUSPS']==j]\n",
        "  site = t_df['Label']\n",
        "  lon = 360+t_df['X_WGS84']\n",
        "  lat = t_df['Y_WGS84']\n",
        "  for i in gcms:\n",
        "    ppt_files = glob.glob('/content/drive/MyDrive/Thesis/Chapter2_simulations/netcdfs/'+j+'/agg_macav2metdata_pr_'+i+'*rcp85_*.nc')\n",
        "    tmax_files = glob.glob('/content/drive/MyDrive/Thesis/Chapter2_simulations/netcdfs/'+j+'/agg_macav2metdata_tasmax_'+i+'*rcp85_*.nc')\n",
        "    tmin_files = glob.glob('/content/drive/MyDrive/Thesis/Chapter2_simulations/netcdfs/'+j+'/agg_macav2metdata_tasmin_'+i+'*rcp85_*.nc')\n",
        "    ## Start with precip -> x\n",
        "    ppt = xr.open_dataset(ppt_files[0])\n",
        "    # Need to get rid of leap years\n",
        "    ppt = ppt.sel(time=~((ppt.time.dt.month == 2) & (ppt.time.dt.day == 29)))\n",
        "    big_ppt = ppt.sel(lon=xr.DataArray(lon), lat=xr.DataArray(lat), method='nearest')\n",
        "    big_ppt = big_ppt.to_array()\n",
        "    big_ppt = big_ppt.reindex(space=['dim_0'])\n",
        "    ppt_df = pd.DataFrame(big_ppt[0,:,:])\n",
        "    t_ppt_df = ppt_df.transpose()\n",
        "    site = t_df['Label']\n",
        "    t_ppt_df['site'] = site\n",
        "    x = pd.DataFrame(t_ppt_df.iloc[:,:-1].values.ravel(), columns = ['PPT'])\n",
        "    years = list(range(2006,2100))\n",
        "    days = list(range(1,366))\n",
        "    site = t_df['Label']\n",
        "    years = [ele for ele in years for j in range(365)]\n",
        "    site =  [ele for ele in site for j in range(365*94)]\n",
        "    x['site'] = site\n",
        "    x['year'] = years*len(ppt_df.axes[1])\n",
        "    x['doy'] = days*(94*len(ppt_df.axes[1]))\n",
        "    ## Now tmin -> y\n",
        "    tmin = xr.open_dataset(tmin_files[0])\n",
        "    # Need to get rid of leap years\n",
        "    tmin = tmin.sel(time=~((tmin.time.dt.month == 2) & (tmin.time.dt.day == 29)))\n",
        "    big_tmin = tmin.sel(lon=xr.DataArray(lon), lat=xr.DataArray(lat), method='nearest')\n",
        "    big_tmin = big_tmin.to_array()\n",
        "    big_tmin = big_tmin.reindex(space=['dim_0'])\n",
        "    tmin_df = pd.DataFrame(big_tmin[0,:,:])\n",
        "    t_min_df = tmin_df.transpose()\n",
        "    site = t_df['Label']\n",
        "    t_min_df['site'] = site\n",
        "    y = pd.DataFrame(t_min_df.iloc[:,:-1].values.ravel(), columns = ['Tmin_C'])\n",
        "    years = list(range(2006,2100))\n",
        "    days = list(range(1,366))\n",
        "    site = t_df['Label']\n",
        "    years = [ele for ele in years for j in range(365)]\n",
        "    site =  [ele for ele in site for j in range(365*94)]\n",
        "    y['site'] = site\n",
        "    y['year'] = years*len(ppt_df.axes[1])\n",
        "    y['doy'] = days*(94*len(tmin_df.axes[1]))\n",
        "    ## Now tmax -> z\n",
        "    tmax = xr.open_dataset(tmax_files[0])\n",
        "    # Need to get rid of leap years\n",
        "    tmax = tmax.sel(time=~((tmax.time.dt.month == 2) & (tmax.time.dt.day == 29)))\n",
        "    big_tmax = tmax.sel(lon=xr.DataArray(lon), lat=xr.DataArray(lat), method='nearest')\n",
        "    big_tmax = big_tmax.to_array()\n",
        "    big_tmax = big_tmax.reindex(space=['dim_0'])\n",
        "    tmax_df = pd.DataFrame(big_tmax[0,:,:])\n",
        "    t_max_df = tmax_df.transpose()\n",
        "    site = t_df['Label']\n",
        "    t_max_df['site'] = site\n",
        "    z = pd.DataFrame(t_max_df.iloc[:,:-1].values.ravel(), columns = ['Tmax_C'])\n",
        "    years = list(range(2006,2100))\n",
        "    days = list(range(1,366))\n",
        "    site = t_df['Label']\n",
        "    years = [ele for ele in years for j in range(365)]\n",
        "    site =  [ele for ele in site for j in range(365*94)]\n",
        "    z['site'] = site\n",
        "    z['year'] = years*len(ppt_df.axes[1])\n",
        "    z['doy'] = days*(94*len(tmax_df.axes[1]))\n",
        "    ## Now merge x,y,z\n",
        "    full = pd.merge(x, y,  on=['site','year','doy'])\n",
        "    full = pd.merge(full, z,  on=['site','year','doy'])\n",
        "    full.to_csv(\"/content/drive/MyDrive/Thesis/Chapter2_simulations/future/\"+j+'_'+i+\"_85.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agVwwF2FDT4I"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-99FDd7q4Tjm"
      },
      "outputs": [],
      "source": [
        "gcms=['bcc-csm1-1','bcc-csm1-1-m','BNU-ESM','CanESM2','CCSM4','CNRM-CM5',\n",
        "      'CSIRO-Mk3-6-0','GFDL-ESM2G','GFDL-ESM2M','HadGEM2-CC365','HadGEM2-ES365',\n",
        "      'inmcm4','IPSL-CM5A-LR','IPSL-CM5A-MR','IPSL-CM5B-LR','MIROC5','MIROC-ESM',\n",
        "      'MIROC-ESM-CHEM','MRI-CGCM3','NorESM1-M']\n",
        "rcps = ['45','85']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now combine all the state files into a single csv"
      ],
      "metadata": {
        "id": "VIb6JsDJy0q0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ok-tI3pW7xDV"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "for j in rcps:\n",
        "  for i in gcms:\n",
        "    files = glob.glob(\"/content/drive/MyDrive/Thesis/Chapter2_simulations/future/*\"+i+\"_\"+j+\".csv\")\n",
        "    for k in files:\n",
        "      x = pd.read_csv(k)\n",
        "      if k == files[0]:\n",
        "        df = x\n",
        "      else:\n",
        "        df = pd.concat([df,x],ignore_index=True)\n",
        "    df.to_csv(\"/content/drive/MyDrive/Thesis/Chapter2_simulations/future/all_sites_\"+i+'_'+j+\".csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}